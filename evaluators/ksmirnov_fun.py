import pickle, os, sys, string
os.chdir(os.getenv('HOME') + '/Documents/Blender')
import numpy as np 
from numpy import matlib 
from scipy import stats
import pandas as pd
from gensim import utils
from sklearn import feature_extraction
from statsmodels.distributions import empirical_distribution as ed
import pdb
from utils import text_fun


nlp = text_fun.nlp
def ks(cdfVec1, cdfVec2):
    """ Computes the Kolmogorov-Smirnov Distance between 2 cdf vectors."""
    return(max(abs(cdfVec1 - cdfVec2)))

def cdf(array, grid_pts = 500):
    """This takes a 2D array (matrix) of jaccard indices and returns 
    the cdf values on a grid with grid_pts points equally spaced from 0 to 1.
    """
    array = np.array(array)
    utVec = np.diagonal(array)
    for i in range(1, array.shape[0]):
        utVec = np.concatenate([utVec, np.diagonal(array, i)])
    xgrid = np.linspace(0, 1, grid_pts)
    ecdf = ed.ECDF(utVec)
    yvals = ecdf(xgrid)
    return(yvals)


def ksFunctionGenerator(text_list, grid_pts = 500):
    """ Takes a list of lists as an argument, where each sub-list has tokens.  
    It returns a new function that evaluates the Kolmogorov-Smirnov distance 
    of a new idea from the baseline CDF generated by text_list."""
    for i, text in enumerate(text_list):
        text_list[i] = ' '.join(text)

    # initializes a counter from sklearn
    vectorizer = feature_extraction.text.CountVectorizer() 
    dtm = vectorizer.fit_transform(text_list) 
    dtm = dtm.toarray()
    vocab = vectorizer.get_feature_names() # get all the words
    dtm = pd.DataFrame(dtm, columns = vocab) 
    tdm = dtm.transpose() # term-doc mat = transpose of doc-term mat
    idx = tdm.sum(axis = 1).sort_values(ascending = False).index 
    tdm = tdm.iloc[idx] # sort the term-doc mat by word frequency
    print(list(tdm.index.values)[0:10])
    totals = tdm.sum(axis = 1)
    wordFreqThreshold = 10
    tdm = tdm[totals > wordFreqThreshold] # remove rows for infrequent words
    totals = totals[totals > wordFreqThreshold]
    tdmInd = tdm > 0
    tdmInd = tdmInd.astype(int)

    interMat = pd.DataFrame.dot(tdmInd, tdmInd.transpose())

    # Union(A, B) = A + B - Intersection(A, B)
    totalsMat = matlib.repmat(np.diagonal(interMat), interMat.shape[0], 1)
    unionMat = totalsMat + np.transpose(totalsMat) - interMat
    vocab = list(interMat.index)

    # Need to remove words in the original lists
    for i, doc in enumerate(text_list):
        temp = utils.simple_preprocess(doc)
        temp = list(set(temp) & set(vocab))
        temp.sort()
        text_list[i] = temp

    # removes empty documents
    text_list = [doc for doc in text_list if doc]
    nums = [i for i in range(0, len(vocab))]
    wordToNum = dict(zip(vocab, nums))
    numToWord = dict(zip(nums, vocab))
    jaccardMat = pd.DataFrame(interMat / unionMat, index = vocab, 
        columns = vocab)

    def jaccard(text):
        index = [wordToNum[word] for word in text if \
            word in wordToNum.keys()]
        out = jaccardMat.iloc[index, index]
        return(out)

    cdfMat = np.zeros([grid_pts, len(text_list)])
    for i, doc in enumerate(text_list):
            cdfMat[:, i] = cdf(jaccard(doc), grid_pts)

    baselineCDF = cdfMat.sum(1) / len(text_list)
        
    def ksEvaluator(tokenList, return_jaccard=False):
        if return_jaccard:
            jaccard_mat = jaccard(tokenList)
            return (ks(cdf(jaccard_mat, grid_pts), baselineCDF), jaccard_mat)
        else:
            return ks(cdf(jaccard(tokenList), grid_pts), baselineCDF)
            
    return ksEvaluator



def ksFunctionGeneratorW2V(text_list, grid_pts = 500):
    """ Takes a list of lists as an argument, where each sub-list has tokens. 
    It returns a new function that evaluates the Kolmogorov-Smirnov distance 
    of a new idea from the baseline CDF generated by text_list.  
    Other version uses Jaccard Indices, this uses w2v similarities from spaCy"""

    for i, text in enumerate(text_list):
            text_list[i] = ' '.join(text)

    # initializes a counter from sklearn
    nlp = text_fun.nlp
    vectorizer = feature_extraction.text.CountVectorizer() 
    dtm = vectorizer.fit_transform(text_list) 
    dtm = dtm.toarray()
    vocab = vectorizer.get_feature_names() # get all the words
    dtm = pd.DataFrame(dtm, columns = vocab) 
    tdm = dtm.transpose() # term-doc mat = transpose of doc-term mat
    idx = tdm.sum(axis = 1).sort_values(ascending = False).index 
    tdm = tdm.ix[idx] # sort the term-doc mat by word frequency
    totals = tdm.sum(axis = 1)
    wordFreqThreshold = 10
    tdm = tdm[totals > wordFreqThreshold] # remove rows for infrequent words
    totals = totals[totals > wordFreqThreshold]
    tdmInd = tdm > 0
    tdmInd = tdmInd.astype(int)
    interMat = pd.DataFrame.dot(tdmInd, tdmInd.transpose())
    vocab = list(interMat.index)
    vocab_string = nlp(' '.join(vocab))
    sim_mat = np.zeros([len(vocab), len(vocab)])
    for i, token_i in enumerate(vocab_string):
        print(i, token_i)
        for j, token_j in enumerate(vocab_string):
            sim_mat[i, j] = token_i.similarity(token_j)

    sim_mat = sim_mat + abs(sim_mat.min())
    sim_mat = sim_mat / sim_mat.max()
    sim_mat = pd.DataFrame(sim_mat, index = vocab)
    vocab = list(sim_mat.index)

    # Need to remove words in the original lists
    for i, doc in enumerate(text_list):
        temp = utils.simple_preprocess(doc)
        temp = list(set(temp) & set(vocab))
        temp.sort()
        text_list[i] = temp

    # removes empty documents
    text_list = [doc for doc in text_list if doc]
    nums = [i for i in range(0, len(vocab))]
    wordToNum = dict(zip(vocab, nums))
    numToWord = dict(zip(nums, vocab))

    def sims(text):
        index = [wordToNum[word] for word in text if \
            word in wordToNum.keys()]
        out = sim_mat.iloc[index, index]
        return(out)

    cdfMat = np.zeros([grid_pts, len(text_list)])
    for i, doc in enumerate(text_list):
            cdfMat[:, i] = cdf(sims(doc), grid_pts)

    baselineCDF = cdfMat.sum(1) / len(text_list)
        
    def ksEvaluator(tokenList, return_sims=False):
        if return_sims:
            sim_mat = sims(tokenList)
            return (ks(cdf(sim_mat, grid_pts), baselineCDF), sim_mat)
        else:
            return ks(cdf(sims(tokenList), grid_pts), baselineCDF)
            
    return ksEvaluator